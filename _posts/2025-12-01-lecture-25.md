---
layout: distill
title: "Lecture 25: Alignment, Explainability, and Open Directions"
description: A summary of the lecture covering the history of interpretability, scaling laws, system design perspectives on AI, and current open problems in deep learning.
date: 2025-12-01
bibliography: 2025-12-01-lecture-25.bib

lecturers:
  - name: Ben Lengerich
    url: "https://www.wisc.edu/"

authors:
  - name: Rishit Malpani
    url: "https://rishit-malpani.github.io/dgm-fall-2025/notes/lecture-25/"

abstract: >
  This lecture explores the transition from raw model performance to the nuances of alignment and explainability. It covers the history of interpretability (from the "mythos" of 2016 to modern mechanistic approaches), scaling laws in foundation models, and a systems design perspective on why interpretability matters—focusing on information acquisition, value alignment, and modularity. The lecture concludes with a look at open problems in the field as we return to an "age of research" with big compute.
---

## Logistics

* **Project Final Report:** Due Friday, December 12th. Submit via Canvas.
* **Final Exam:** December 17th, 5:05-7:05 PM in Science 180. A study guide has been released.

---

## The LLM Pipeline

Before diving into alignment, it is crucial to understand the lifecycle of a modern Large Language Model (LLM). [cite_start]The process moves through distinct stages [cite: 18-33]:

1.  **Pre-Training:** Starting with a random model, we train on massive datasets (e.g., Common Crawl) to create a "Pre-Trained Model".
2.  **Fine-Tuning:** The model undergoes Supervised Fine-Tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF) using in-domain data to become a "Fine-Tuned Model".
3.  **In-Context Learning:** Finally, prompting strategies allow the model to adapt to specific tasks without weight updates.

---

## The History of Interpretability

### 2016: Setting the Stage
In the early days of deep learning popularity, interpretability was often invoked vaguely when metrics failed to capture the full picture.
* **The Mythos:** Lipton (2016) noted that "Interpretability is invoked when metrics <d-math>\neq</d-math> objectives" <d-cite key="lipton2018mythos"></d-cite>.
* **Modes of Evaluation:** Doshi-Velez & Kim (2017) formalized three evaluation modes: *Application-grounded*, *Human-grounded*, and *Functionally-grounded* <d-cite key="doshi2017towards"></d-cite>.

### 2017-2020: Fragmented Approaches
[cite_start]The field split into distinct methodologies[cite: 70]:

| Methodology | Examples | Description |
| :--- | :--- | :--- |
| **Post-Hoc** | LIME, SHAP, Integrated Gradients | Became industry standard; explains model after training. |
| **Transparency** | GAMs, Monotonic Nets | Niche use in healthcare/tabular data; inherently interpretable. |
| **Mechanistic** | Circuits, probing | Technically deep (e.g., feature geometry), but rarely user-facing. |

### Cracks in Post-Hoc Explanations
Research eventually highlighted significant issues with post-hoc saliency maps.
* [cite_start]**Fidelity:** Popular tools look convincing but often lack fidelity to the underlying model[cite: 74].
* **Insensitivity:** Adebayo et al. (2018) demonstrated that many saliency maps look the same even when model weights are randomized <d-cite key="adebayo2018sanity"></d-cite>.
* **Vulnerability:** Slack et al. (2020) showed that LIME and SHAP are easily fooled <d-cite key="slack2020fooling"></d-cite>.

---

## Foundation Models and Scaling Laws

From 2017 (Transformers) to 2024 (GPT-4/5), the field has been dominated by scaling. Kaplan et al. (2020) demonstrated that performance is not random; it follows strict **power-law relationships** with three factors: Compute, Dataset Size, and Parameters <d-cite key="kaplan2020scaling"></d-cite>.

For optimal performance, all three must be scaled in tandem. [cite_start]When not bottlenecked by the other two, the loss <d-math>L</d-math> scales as follows[cite: 136, 154, 184]:

**Compute (<d-math>C</d-math>):**

<d-math block>
L(C) = (C_{min} / 2.3 \cdot 10^8)^{-0.050}
</d-math>

**Dataset Size (<d-math>D</d-math>):**

<d-math block>
L(D) = (D / 5.4 \cdot 10^{13})^{-0.095}
</d-math>

**Parameters (<d-math>N</d-math>):**

<d-math block>
L(N) = (N / 8.8 \cdot 10^{13})^{-0.076}
</d-math>

---

## A System Design View of Interpretability

Why do we need interpretability? We can view it through the lens of **System Design**. [cite_start]Just as in basketball, where "Net Rating" (a system stat) correlates better with winning than "Points Per Game" (an individual stat), interpretability allows us to optimize the human-AI system rather than just the model's accuracy[cite: 216].

[cite_start]There are three primary benefits to this view: **Information Acquisition**, **Value Alignment**, and **Modularity** [cite: 220-223].

### 1. Information Acquisition
Predictive models usually treat measurements as fixed, but in fields like biomedicine, measurement is costly and active. Interpretability can highlight *what* we should be measuring.

**Case Study: Severe Maternal Morbidity (SMM)**
Researchers used a Generalized Additive Model (GAM) to predict SMM. A GAM decomposes complex outcomes into univariate functions <d-cite key="hastie1990generalized"></d-cite>:

<d-math block>
F(x) = \beta_0 + f_1(x_1) + f_2(x_2) + ... + f_r(x_r)
</d-math>

The model revealed a strange step-function risk regarding **Maternal Height** <d-cite key="lengerich2021insights"></d-cite>. Upon investigation, it was discovered that the model wasn't using height directly, but rather using it as a proxy for the **BabySize-MaternalHeight Ratio (BMSR)**:

<d-math block>
\text{BMSR} = \frac{\text{Birth Weight (kg)}}{\text{Maternal Height}^2}
</d-math>

[cite_start]This derived feature was the #1 most important feature—more than preeclampsia [cite: 408-410]. Interpretability helped acquire new medical knowledge.

### 2. Modularity
Interpretability allows us to view models as collections of swappable, testable components rather than monolithic black boxes.
* **Reverse Engineering:** We can extract components from trained models using concept-based interpretability <d-cite key="concept2025interpretability"></d-cite>.
* **Goal:** Incorporate these modular components from the start of training to allow for easier testing and updating.

### 3. Value Alignment
[cite_start]We must distinguish between **Outer Alignment** (Is the loss function aligned with human goals?) and **Inner Alignment** (Does the model faithfully implement the goal?) [cite: 448-449].

[cite_start]A major challenge is the **Jagged Frontier**: AI can be "unbelievably intelligent" at some tasks while failing at trivial ones [cite: 454-459].

**Goodhart's Law in Medicine**
"When a measure becomes a target, it ceases to be a good measure."
In medical AI, we see that treatments confound data.
* *Example:* A model predicting mortality in pneumonia patients might learn that extremely high Creatinine levels correlate with *lower* risk <d-cite key="lengerich2025hidden"></d-cite>.
* *Why?* Because doctors identify those patients as high-risk and give them dialysis. The model learns the outcome of the *intervention*, not the biological risk.
* [cite_start]*Conclusion:* If we used this biomarker to guide treatment (stopping dialysis because risk looks low), the metric would cease to predict the outcome[cite: 560].

---

## Open Problems

[cite_start]We are returning to an "age of research," but with bigger computers[cite: 605]. Some key open problems include:

1.  **Jaggedness:** Models have high eval performance but lack robust economic impact due to reliability issues.
2.  **Verifiable Rewards:** Scaling Reinforcement Learning (RL) requires better, verifiable reward signals.
3.  **Reasoning:** Combining LLMs with symbolic reasoning and graphical models.
4.  **Alignment Theory:** Developing a formal theory of alignment and "ante-hoc" interpretable-by-design models.
5.  **Data Walls:** Pre-training scales uniformly but is hitting the limits of available data.
