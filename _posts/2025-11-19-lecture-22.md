---
layout: distill
title: Lecture 22
description: Unsupervised Training of LLMs
date: 2025-11-29

lecturers:
  - name: Ben Lengerich
    url: "https://adaptinfer.org"

authors:
  - name: Zefan Cai
  - name: Wenjie Hu
  - name: Lucy Xian
  - name: Jack Charles Cannon

editors:
  - name: 

---
## 1. Unsupervised Training of LLMs (Overview)

### Goal
Train large language models (LLMs) using maximum likelihood estimation (MLE) on raw text—no labels, no supervision, no explicit objectives beyond predicting the next token.

### Context (From GPT-1 → GPT-4)

LLMs have grown dramatically in:

* **Scale**: 1.5B parameters → over 1T parameters
* **Context length**: 512 → 128k tokens
* **Depth & width**: 12 → 96+ layers; 12 → 96+ heads
* **Embedding dimension**: 768 → over 12k
* **Vocab**: 40k → 50k+
* **Training data**: 5GB BookCorpus → private 13T tokens (~50 TB)

Modern tokenizers often include image patches, enabling multimodality.
Mixture-of-Experts architectures further boost scale.
Training now also includes reinforcement learning for alignment after pretraining.

## 2. MLE Training of Language Models

### MLE Objective

LLMs are trained to maximize:

$$
\hat{\theta}_{\text{MLE}} = \arg\max_\theta \mathbb{E}_{x\sim P_{\text{data}}}[\log P_\theta(x)]
$$

They model sequences autoregressively:

$$
P_\theta(X) = \prod_i \prod_t P_\theta(X_{i,t} \mid X_{i,<t})
$$

### Interpretation
The model directly fits the data distribution without introducing structured latent variables or explicitly controlling entropy.

### What MLE Does Well

* Directly models empirical sequences
* Efficient and scalable
* Allows training on extremely large corpora

### But MLE Has Hidden Problems

* Implicit low-entropy bias → overconfident predictions
* Mode collapse / degeneration / memorization
* **Neural text degeneration (Holtzman 2020)**
  * e.g., repeated phrases
  * generic completions
  * "The man said the man said…"
  * lack of diversity unless sampling is carefully tuned

## Scale & Emergent Capabilities

### 3.1 Emergent Capabilities in LLMs

#### What Happens as We Scale Training?

<figure id="scaling-laws" class="l-body-outset">
  <div class="row">
    <div class="col" style="width: 100%; float: left;">
      <img src="{{ '/assets/img/notes/lecture-22/3.1.png' | relative_url }}" />
      <figcaption>
        <strong>Figure 1.</strong>
        Language modeling performance improves smoothly as we increase
        the model size, dataset size, and amount of compute used for training.
        For optimal performance all three factors must be scaled up in tandem.
        Empirical performance has a power-law relationship with each individual
        factor when not bottlenecked by the other two.
      </figcaption>
    </div>
  </div>
</figure>

<figure id="emergent-abilities" class="l-body-outset">
  <div class="row">
    <div class="col" style="width: 100%; float: left;">
      <img src="{{ '/assets/img/notes/lecture-22/3.2.png' | relative_url }}" />
      <figcaption>
        <strong>Figure 2.</strong>
        Smooth improvements in overall loss can lead to sharp “emergent”
        jumps in task performance. An ability is called emergent if it is
        not present in smaller models but appears in larger models
        (Wei et al., 2022).
      </figcaption>
    </div>
  </div>
</figure>

#### Examples of Emergence

1. **In-Context Learning** : Model learns to perform tasks from examples provided in the prompt.
2. **Chain-of-Thought Reasoning** :  Model generates intermediate reasoning steps.
3. **Factual structure understanding**: To fill in “The capital of France ___ Paris”, model must: 
    - Track subject-verb agreement;
    - Understand the underlying fact structure;
    - Recognize task domain (geography).

#### Hypotheses for Emergence

- Scale increases representational capacity  
- MLE forces models to capture global dependencies  
- Large training corpora contain implicit demonstrations of reasoning  
- Transformers act as meta-learners over massive data distributions


### 3.2 Challenges in Scaling Unsupervised Training

#### Data Filtering Issues

At trillion-token scale, filtering toxic, low-quality, or duplicated text becomes extremely difficult.

#### Parallel Training

Training trillion-parameter models requires:
- Expert parallelism
- Distributed optimization
- Fault tolerance
- Training pipelines like DeepSpeed / Megatron

#### LLM360 Initiative

Open efforts like LLM360 / TxT360 aim to make complete training pipelines observable and reproducible for education and research.

---

<footer>
<p>© 2025 University of Wisconsin — STAT 453 Lecture Notes</p>
</footer>

